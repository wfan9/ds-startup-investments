---
title: "Predicting Startup Status"
author: "William Fang"
date: "15/06/2020"
geometry: "left=3cm,right=3cm,top=2cm,bottom=2cm"
bibliography: references-dl.bib
link-citations: true
nocite: '@*'
output:
  pdf_document:
    toc: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(include = FALSE, echo = FALSE, warning = FALSE, message = FALSE, error = FALSE, cache = TRUE, comment = NA)

# Save/load computationally expensive data to/from file to save time during development.
use_cache <- FALSE
```

```{r download-bibliography}
#download.file(
```

# Introduction

@@@@ Rework to focus on this dataset, not representative of startups in general, put that in results
discussion. Also not well defined the time range.

While "Unicorn" startups (companies valued at over \$1 billion) receive the most coverage, the vast majority
of startups fail. Various statistics on the failure rate can be found on the internet, (90% according to
@startup-fail-90), but in general terms the odds of success are small. The aim of this report is to develop a
model to predict the status of a startup based primarily on the funding of the company. The data set used is
"StartUp Investments (Crunchbase)", available from Kaggle. It provides the status of a company (acquired,
operating, or closed), funding and other feature data.

@@@@ Section that describes the dataset and variables, and summarizes the goal of the project and key steps that were performed.

Steps:

    Load and clean the data.

    Split into training and validation sets.

    Explore the data.

    Create and train models, 

    Test against validation set


```{r required-packages, cache = FALSE}
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(gridExtra)) install.packages("gridExtra", repos = "http://cran.us.r-project.org")
if(!require(stringr)) install.packages("stringr", repos = "http://cran.us.r-project.org")
if(!require(stringi)) install.packages("stringi", repos = "http://cran.us.r-project.org")
```

```{r data-setup}

dl <- tempfile()
download.file("https://github.com/wfan9/ds-startup-investments/raw/master/data/517018_952128_compressed_investments_VC.csv.zip", dl)
data <- read_csv(unzip(dl))

str(data)

rm(dl)
```
The data set can be downloaded as a zipped CSV file, and has been added to the github repository of this
project. The uncompressed file is approximately 12Mb, and there are `r nrow(data)` rows, and
`r length(names(data))` variables.

## Variables

The variable names are as below:

```{r variables-table, include = TRUE}
names(data)
```

The data set has no meta data explaining the definitions of the features, and instead reasonable
interpretations are assumed.

### Market and Categories

The `market` variable is string representing the main market the startup is targeting.
`category_list` contains one or more categories the startup belongs to. Each category is separated by a |, and
there is no specific ordering to the list.

```{r category_list-sample, include = TRUE}
kable(head(data$category_list), caption = "Sample category_list Values")
```

### Location

There are four variables that describe location.  `country_code` which is a 3 character string, `state_code` a
2 character string for companies within the US, `region` and `city` which are strings.

### Status

The focus of this report is to create a model for predicting status. The variable `status` has the following values:

```{r status-values, include = TRUE}
kable(unique(data$status), caption = "Status Values")
```
There are companies with a missing status value, these will be removed as they can't be used.

### Date

@@@@ Describe

founded_at
founded_month
founded_quarter
founded_year
first_funding_at
last_funding_at

### Funding

@@@@ Describe

funding_total_usd
funding_rounds
seed
venture
equity_crowdfunding
undisclosed
convertible_note
debt_financing
angel
grant
private_equity
post_ipo_equity
post_ipo_debt
secondary_market
product_crowdfunding

round_A
round_B
round_C
round_D             
round_E
round_F
round_G
round_H


### Ignored Variables

The following variables will be ignored, as they are very unlikely to have any predictive power. `permalink`,
which is a hyperlink from the Techcrunch data, and `homepage_url`, which is the company's web page.

Startup `name`, there may be some sort of predictive power here. For example, a catchy or memorable startup name
might attract more attention and lead to more investment, consumer interest etc. However in terms of this
dataset it isn't likely to be useful.

# Analysis

## Data Cleaning

The first step is to examine the data, looking for missing data and bad/invalid values, and wrong data types.

### Missing Values

The following table shows the counts of missing (NA) values in the data set:

```{r missing-values-counts-table, include = TRUE}
missing_values <- data %>%
    gather(key = "variable", value = "val") %>%
    mutate(missing = is.na(val)) %>%
    group_by(variable, missing) %>%
    summarise(count = n()) %>%
    filter(missing==TRUE) %>%
    select(-missing) %>%
    arrange(desc(count)) 

kable(missing_values, caption = "Variables With Missing Values Counts")
rm(missing_values)
```
    
There are a few variables where there is only 1 NA value, and it turns out they all belong to one entry which
can be removed.

```{r remove-bad-row}
data <- data %>% filter(!is.na(angel))
```

The following plot shows the percentage of a variable that has NA values (code adapted from @vis-miss-value):

```{r missing-values-plot, include = TRUE}
# Count how many NAs per variable type, then work out percentage.
missing_values <- data %>%
  gather(key = "key", value = "val") %>%
  mutate(isna = is.na(val)) %>%
  group_by(key) %>%
  mutate(total = n()) %>%
  group_by(key, total, isna) %>%
  summarise(num_isna = n()) %>%
  mutate(pct = num_isna / total * 100)

# Order so that least missing are first, this will be used as scale labels.
levels <- (missing_values %>% filter(isna == TRUE) %>% arrange(desc(pct)))$key

percentage_plot <- missing_values %>%
  ggplot() +
  geom_bar(aes(x = reorder(key, desc(pct)), y = pct, fill=isna),  stat = 'identity', alpha=0.8) +
  scale_x_discrete(limits = levels) +
  scale_fill_manual(name = "", values = c('steelblue', 'tomato3'), labels = c("Present", "Missing")) +
  coord_flip() +
  labs(title = "Percentage of missing values", x = 'Variable', y = "% of missing values")

percentage_plot
rm(levels, percentage_plot)
#rm(missing_values, levels, percentage_plot)
```

Handling of NA values for the rest of the variables will be discussed in the following sections.

### Status

Since the aim is to predict the status of a company, the rows that don't have a valid status can't be used and
are simply removed. The data type is changed from character to factor. Then the counts of the 3 statuses can
be seen:

```{r clean-transform-status}
data <- data %>% filter(!is.na(status)) %>% mutate(status = factor(status))
```

### Unused Variables

```{r duplicate-name-counts}
duplicate_name_counts <-
data %>% group_by(name) %>%
 summarise(n = n()) %>%
 filter(n > 1)
```

The features `name`, `permalink`, and `homepage_url` are unlikely to have predictive power. However it will be
useful to have a unique ID to identify rows. Examining the names shows there are actually
`r nrow(duplicate_name_counts)` names that occur more than once. Some of them are due to duplicated/erroneous
data, while others are due to different companies having the same name. Cleaning of bad rows is done manually
by looking at each set of duplicate rows. From first look at the duplicate rows, it is common for the
`homepage_url` to be the same, so this was used to help reduce the number of rows that needed to be manually
inspected.

In light of names not being unique, and not all rows having a `homepage_url`, `permalink` will be used as the
unique ID. However checking `permalink` for duplicates also shows one pair with the same value, which also is
removed. The `name` and `homepage_url` will not be removed yet as they may help with cleaning of other
features.

```{r remove-duplicate-rows}
# Show the rows for manual inspection.
data %>% filter(name %in% duplicate_name_counts$name)

# Duplicate rows are often indicated by the same home_page
duplicate_homepages <- data %>%
  filter(name %in% duplicate_name_counts$name) %>%
  group_by(homepage_url) %>%
  summarise(n = n()) %>%
  filter(n > 1)

# Show rows that are in the duplicate names and have duplicate homepages. This also brings up rows
# with NA for homepage_url, which are rows that need examining as well.
data %>% filter(homepage_url %in% duplicate_homepages$homepage_url & name %in% duplicate_name_counts$name)

# permalink's of rows that look like bad data from manual inspection.
bad_permalink <- c("/organization/adometry-2", "/organization/ayoxxa-biosystems-2",
"/organization/busportal-2", "/organization/cgtrader-2", "/organization/cue-4", "/organization/gain-fitness",
"/organization/checkpoints", "/organization/lightex-ltd-", "/organization/realync-2",
"/organization/trippeace", "/organization/university-of-rochester-2", "/organization/victiv")
data <- data %>% filter(!(permalink %in% bad_permalink))

# Special case where the permalink is also duplicated, the only way to distinguish between the
# two is based on another field, such as market.
data <- data %>% filter(!(permalink == "/organization/treasure-valley-urology-services" & is.na(market)))

# Check for duplicate permalink values not removed during above process.
data %>% group_by(permalink) %>% summarise(n = n()) %>% filter(n > 1)

# After manually examining the output, use the following to remove the bad row:
data <- data %>% filter(!(permalink == "/organization/prysm" & is.na(market)))
```

### Market and Categories

There are `r n_distinct(data$market)` unique values for market, including NA values which will be replaced with
the string "Unknown".

```{r clean-market}
data <- data %>% mutate(market = str_replace_na(market, "Unknown"))
all_markets <- unique(data$market)
min_count <- 250
```

NA values for `category_list` will be treated the same as for `market` by replacing them with "Unknown".
Looking at the data shows there are 6 rows where there is truncation, however these will not be changed
since there are only a few.

```{r category_list-clean}
data <- data %>% mutate(category_list = str_replace_na(category_list, "|Unknown|"))

# Look for values that don't match expected pattern. There are 6 which appear truncated, which we don't do
# anything about.
data %>% filter(!str_detect(category_list, "^\\|.+\\|$")) %>% .$category_list
```

```{r split-category_list}
category_data <- data %>% separate_rows(category_list, sep = "\\|") %>%
  filter(str_length(category_list) > 0) %>%
  mutate(category = category_list)
all_categories <- unique(category_data$category)
```

The `category_list` field needs to be split into individual categories to look for status - category
relationships. There are `r length(all_categories)` categories.

### Date

The columns `founded_year`, `founded_quarter`, and `founded_month` should have been derived from `founded_at`,
so these will be removed and recreated if needed from the cleaned `founded_at` variable.
```{r remove-derived-date-vars}
data <- data %>% select(-founded_year, -founded_quarter, -founded_month)
```

```{r show-date-ranges, include = TRUE}
summarise_dates <- function() {
  data %>% filter(!is.na(founded_at)) %>%
    summarise(min_founded = min(founded_at),
              max_founded = max(founded_at),
              min_first_fund = min(first_funding_at),
              max_first_fund = max(first_funding_at),
              min_last_fund = min(last_funding_at),
              max_last_fund = max(last_funding_at))
}

summarise_dates() %>% kable(caption = "Min/Max Dates")
```

There are "startups" that were founded in the 1800's. Companies that old are not likely to be considered
startups. Somewhat arbitrarily, restrict the range to companies founded in or after the 1990's.

It can be seen that `first_funding_at` and `last_funding_at` have invalid values, which need correction. In
additon there are companies founded in the 1800's.

```{r check-date-format-founded_at}
# Show any rows that have a bad founded_at value
data %>% filter(!is.na(founded_at)) %>% filter(!is_valid_date(founded_at))
```

```{r check-date-format-first_funding_at, include = TRUE}
is_valid_date <- function(d) {
  str_detect(d, "^[0-9]{4}-[0-9]{2}-[0-9]{2}$")
}
data %>% filter(!is.na(first_funding_at)) %>%
  filter(!is_valid_date(first_funding_at)) %>%
  select(permalink, founded_at, first_funding_at, last_funding_at) %>%
  kable(caption = "Bad Date Format first_funding_at")
```

```{r check-date-format-last_funding_at, include = TRUE}
data %>% filter(!is.na(last_funding_at)) %>%
  filter(!is_valid_date(last_funding_at)) %>%
  select(permalink, founded_at, first_funding_at, last_funding_at) %>%
  kable(caption = "Bad Date Format last_funding_at")
```

These show 10 rows that will manually be corrected.
```{r fix-funding-dates}
data[which(data$permalink == "/organization/fitfrnd-2"), "first_funding_at"] <- as_datetime("2014-11-26")
data[which(data$permalink == "/organization/fitfrnd-2"), "last_funding_at"] <- as_datetime("2014-11-26")
data[which(data$permalink == "/organization/agflow"), "first_funding_at"] <- as_datetime("2013-06-01")
data[which(data$permalink == "/organization/buru-buru"), "first_funding_at"] <- as_datetime("2013-04-01")
data[which(data$permalink == "/organization/exploco"), "first_funding_at"] <- as_datetime("2014-10-01")
data[which(data$permalink == "/organization/exploco"), "last_funding_at"] <- as_datetime("2014-10-01")
data[which(data$permalink == "/organization/nubank"), "first_funding_at"] <- as_datetime("2013-05-07")
data[which(data$permalink == "/organization/peoplegoal"), "first_funding_at"] <- as_datetime("2014-05-01")
data[which(data$permalink == "/organization/peoplegoal"), "last_funding_at"] <- as_datetime("2014-05-01")
data[which(data$permalink == "/organization/rotor"), "first_funding_at"] <- as_datetime("2014-09-29")
data[which(data$permalink == "/organization/rotor"), "last_funding_at"] <- as_datetime("2014-09-29")
data[which(data$permalink == "/organization/securenet-payment-systems"), "first_funding_at"] <- as_datetime("1997-01-01")
data[which(data$permalink == "/organization/shopboostr"), "first_funding_at"] <- as_datetime("2014-11-01")
data[which(data$permalink == "/organization/shopboostr"), "last_funding_at"] <- as_datetime("2014-11-01")
data[which(data$permalink == "/organization/the-urban-roosters"), "first_funding_at"] <- as_datetime("2014-07-01")
data[which(data$permalink == "/organization/the-urban-roosters"), "last_funding_at"] <- as_datetime("2014-07-01")

summarise_dates() %>% kable(caption = "Corrected Min/Max Dates")
```

```{r remove-old-companies}
data <- data %>% filter(founded_at >= "1990-01-01")
```


A first step is to check that the strings match expected format `YYYY-MM-DD`. All non-NA values for `founded_at` match this.

At this point it isn't obvious how NA values should be dealt with.
```{r convert-to-datetime}
#data <- data %>% mutate(founded_at = as_datetime(founded_at),
foo <- data %>% mutate(founded_at = as_datetime(founded_at),
                        first_funding_at = as_datetime(first_funding_at),
                        last_funding_at = as_datetime(last_funding_at)) %>%
  mutate(founded_year = round_date(founded_at, unit = "year"),
         founded_quarter = round_date(founded_at, unit = "quarter"),
         founded_month = round_date(founded_at, unit = "month"))

# Check dates are in the correct order
data %>% filter(first_funding_at > last_funding_at)
```


@@@@ 
* Use set operations to determine that category values are a super set of market values.
* To use for prediction, need to spread each value of category becomes a feature, with True or False. Do a small example of the resulting matrix, but will not actually do it fully as
the size will be huge. Split the categories and print out the number of unique categories, and perhaps the
histogram of the categories. Perhaps a reduced set of categories that occur frequently enough can be used.

```{r removed-unused-features}
#data <- data %>% select(-permalink, -homepage_url)
```

## Data Exploration

### Status

The three status values have the following counts:

```{r status-hist-plot, include = TRUE}
data %>% group_by(status) %>%
  summarise(count = n()) %>%
  ungroup() %>%
  mutate(percentage = count * 100 / sum(count)) %>%
  kable(digits = 1, caption = "Status Counts")
  #ggplot(aes(data$status)) + geom_histogram(stat = "count")
```

Close to 87% of companies in this dataset are operating. This is not representitive of startups in general
where @@@@ duplicate info?.

### Market and Categories

The following histogram plot shows the counts of the status per market (filtered for markets where there are
at least `r min_count` companies).

```{r market-status-plot, include = TRUE}
# Change colours for graphing so that acquired is marked with green, closed with red, and operating blue.
status_colours <- c("#00BA38", "#F8766D", "#619CFF")

market_data <- data %>% group_by(market) %>%  
  filter(n() >= min_count) %>%
  # We want to reorder in descending order
  mutate(n  = n()) %>%
  ungroup() %>%
  mutate(market = reorder(market, n))

market_data %>%
  ggplot(aes(market, fill = status)) +
  geom_histogram(stat = "count") +
  scale_fill_manual(values = status_colours) +
  coord_flip()
rm(min_count)
```

The above plot shows the proportion of statuses are not uniform and vary by market type. This is better
visualized by plotting percentages of each status per market:

```{r market-status-proportion-plot, include = TRUE}
market_status_percentages <- market_data %>% group_by(market) %>%
  mutate(n_market = n()) %>%
  group_by(market, status) %>%
  mutate(percentage = n() * 100/ n_market) %>%
  select(market, status, percentage) %>%
  unique()
  
market_status_percentages %>%
  ggplot(aes(market, percentage, fill = status)) +
  geom_bar(stat = "identity") +
  scale_fill_manual(values = status_colours) +
  coord_flip()

rm(market_data)
```

Creating plots similar to those for `market`:

```{r category-status-plot, include = TRUE}
min_count <- 750
cat_data <- category_data %>% group_by(category) %>%  
  filter(n() >= min_count) %>%
  # We want to reorder in descending order
  mutate(n  = n()) %>%
  ungroup() %>%
  mutate(category = reorder(category, n))

cat_data %>%
  ggplot(aes(category, fill = status)) +
  geom_histogram(stat = "count") +
  scale_fill_manual(values = status_colours) +
  coord_flip()
rm(min_count)
```

```{r category-status-proportion-plot, include  = TRUE}
category_status_percentages <- cat_data %>% group_by(category) %>%
  mutate(n_category = n()) %>%
  group_by(category, status) %>%
  mutate(percentage = n() * 100/ n_category) %>%
  select(category, status, percentage) %>%
  unique()
  
category_status_percentages %>%
  ggplot(aes(category, percentage, fill = status)) +
  geom_bar(stat = "identity") +
  scale_fill_manual(values = status_colours) +
  coord_flip()

rm(category_data, cat_data)
#mark_per %>%
  #left_join(cat_per, by = c("market" = "category", "status" = "status")) %>%
  #arrange(market)
```

To use categories for prediction, they need to be spread so that each category is a feature (with type
logical). 

```{r market-vs-category-values}


```

### Date

@@@@ Turn status, country, state etc into factors. Dates from strings to datetime.

@@@@ What to use for measuring accuracy/loss function?

## Models

Given the prevalance of companies with the status of opearting is 87%, just guessing the status as operating
would give us this accuracy, but with no ability to predict for the other statuses.

@@@@ section that explains the process and techniques used, including data cleaning, data exploration and visualization, any insights gained, and your modeling approach. At least two different models or algorithms must be used, with at least one being more advanced than simple linear regression for prediction problems.

# Results 

@@@@ A results section that presents the modeling results and discusses the model performance.

# Conclusion

@@@@ A conclusion section that gives a brief summary of the report, its potential impact, its limitations, and future work.


# Bibliography

