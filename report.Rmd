---
title: "Predicting Startup Status"
author: "William Fang"
date: "15/06/2020"
geometry: "left=3cm,right=3cm,top=2cm,bottom=2cm"
bibliography: references-dl.bib
link-citations: true
nocite: '@*'
output:
  pdf_document:
    toc: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(include = FALSE, echo = FALSE, warning = FALSE, message = FALSE, error = FALSE, cache = TRUE, comment = NA)

# Save/load computationally expensive data to/from file to save time during development.
use_cache <- FALSE
```

```{r download-bibliography}
#download.file(
```

# Introduction

@@@@ Rework to focus on this dataset, not representative of startups in general, put that in results
discussion. Also not well defined the time range.

While "Unicorn" startups (companies valued at over \$1 billion) receive the most coverage, the vast majority
of startups fail. Various statistics on the failure rate can be found on the internet, (90% according to
@startup-fail-90), but in general terms the odds of success are small. The aim of this report is to develop a
model to predict the status of a startup based primarily on the funding of the company. The data set used is
"StartUp Investments (Crunchbase)", available from Kaggle. It provides the status of a company (acquired,
operating, or closed), funding and other feature data.

@@@@ Section that describes the dataset and variables, and summarizes the goal of the project and key steps that were performed.

Steps:

    Load and clean the data.

    Split into training and validation sets.

    Explore the data.

    Create and train models, 

    Test against validation set


```{r required-packages, cache = FALSE}
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(gridExtra)) install.packages("gridExtra", repos = "http://cran.us.r-project.org")
```

```{r data-setup}

dl <- tempfile()
download.file("https://github.com/wfan9/ds-startup-investments/raw/master/data/517018_952128_compressed_investments_VC.csv.zip", dl)
data <- read_csv(unzip(dl))

str(data)

rm(dl)
```
The data set can be downloaded as a zipped CSV file, and has been added to the github repository of this
project. The uncompressed file is approximately 12Mb, and there are `r nrow(data)` rows, and
`r length(names(data))` variables.

## Variables

The variable names are as below:

```{r variables-table, include = TRUE}
names(data)
```

The data set has no meta data explaining the definitions of the features, and instead reasonable
interpretations are assumed.

### Market and Categories

The `market` variable is string representing the main market the startup is targeting.
`category_list` contains one or more categories the startup belongs to. Each category is separated by a |, and
there is no specific ordering to the list.

```{r category_list-sample, include = TRUE}
kable(head(data$category_list), caption = "Sample category_list Values")
```

### Location

There are four variables that describe location.  `country_code` which is a 3 character string, `state_code` a
2 character string for companies within the US, `region` and `city` which are strings.

### Status

The focus of this report is to create a model for predicting status. The variable `status` has the following values:

```{r status-values, include = TRUE}
kable(unique(data$status), caption = "Status Values")
```
There are companies with a missing status value, these will be removed as they can't be used.

### Date

@@@@ Describe

founded_at
founded_month
founded_quarter
founded_year
first_funding_at
last_funding_at

### Funding

@@@@ Describe

funding_total_usd
funding_rounds
seed
venture
equity_crowdfunding
undisclosed
convertible_note
debt_financing
angel
grant
private_equity
post_ipo_equity
post_ipo_debt
secondary_market
product_crowdfunding

round_A
round_B
round_C
round_D             
round_E
round_F
round_G
round_H


### Ignored Variables

The following variables will be ignored, as they are very unlikely to have any predictive power. `permalink`,
which is a hyperlink from the Techcrunch data, and `homepage_url`, which is the company's web page.

Startup `name`, there may be some sort of predictive power here. For example, a catchy or memorable startup name
might attract more attention and lead to more investment, consumer interest etc. However in terms of this
dataset it isn't likely to be useful.

# Analysis

## Data Exploration And Analysis

The first step is to examine the data, looking for missing data, bad/invalid values, and possible
transformations that will improve the accuracy of the classification model being developed.

### Missing Values

The following table shows the counts of missing (NA) values in the data set:

```{r missing-values-counts-table, include = TRUE}
missing_values <- data %>%
    gather(key = "variable", value = "val") %>%
    mutate(missing = is.na(val)) %>%
    group_by(variable, missing) %>%
    summarise(count = n()) %>%
    filter(missing==TRUE) %>%
    select(-missing) %>%
    arrange(desc(count)) 

kable(missing_values, caption = "Variables With Missing Values Counts")
rm(missing_values)
```
    
There are a few variables where there is only 1 NA value, and it turns out they all belong to one entry which
can be removed.

```{r remove-bad-row}
data <- data %>% filter(!is.na(angel))
```

The following plot shows the percentage of a variable that has NA values (code adapted from @vis-miss-value):

```{r missing-values-plot, include = TRUE}
# Count how many NAs per variable type, then work out percentage.
missing_values <- data %>%
  gather(key = "key", value = "val") %>%
  mutate(isna = is.na(val)) %>%
  group_by(key) %>%
  mutate(total = n()) %>%
  group_by(key, total, isna) %>%
  summarise(num_isna = n()) %>%
  mutate(pct = num_isna / total * 100)

# Order so that least missing are first, this will be used as scale labels.
levels <- (missing_values %>% filter(isna == TRUE) %>% arrange(desc(pct)))$key

percentage_plot <- missing_values %>%
  ggplot() +
  geom_bar(aes(x = reorder(key, desc(pct)), y = pct, fill=isna),  stat = 'identity', alpha=0.8) +
  scale_x_discrete(limits = levels) +
  scale_fill_manual(name = "", values = c('steelblue', 'tomato3'), labels = c("Present", "Missing")) +
  coord_flip() +
  labs(title = "Percentage of missing values", x = 'Variable', y = "% of missing values")

percentage_plot
#rm(missing_values, levels, percentage_plot)
```

Handling of NA values for the rest of the variables will be discussed in the following sections.

### Status

Since the aim is to predict the status of a company, the rows that don't have a valid status can't be used and
are simply removed. The data type is changed from character to factor. Then the counts of the 3 statuses can
be seen:

```{r clean-transform-status}
data <- data %>% filter(!is.na(status)) %>% mutate(status = factor(status))
```

```{r status-hist-plot, include = TRUE}
data %>% group_by(status) %>%
  summarise(count = n()) %>%
  ungroup() %>%
  mutate(percentage = count * 100 / sum(count)) %>%
  kable(digits = 1, caption = "Status Counts")
  #ggplot(aes(data$status)) + geom_histogram(stat = "count")
```

Close to 87% of companies in this dataset are operating. This is not representitive of startups in general
where @@@@ duplicate info?.

### Unused Variables

```{r duplicate-name-counts}
duplicate_name_counts <-
data %>% group_by(name) %>%
 summarise(n = n()) %>%
 filter(n > 1)
```

The features `name`, `permalink`, and `homepage_url` are unlikely to have predictive power. However it will be
useful to have a unique ID to identify rows. Examining the names shows there are actually
`r nrow(duplicate_name_counts)` names that occur more than once. Some of them are due to duplicated/erroneous
data, while others are due to different companies having the same name. Cleaning of bad rows is done manually
by looking at each set of duplicate rows. From first look at the duplicate rows, it is common for the
`homepage_url` to be the same, so this was used to help reduce the number of rows that needed to be manually
inspected.

In light of names not being unique, and not all rows having a `homepage_url`, `permalink` will be used as the
unique ID. However checking `permalink` for duplicates also shows one pair with the same value, which also is
removed. The `name` and `homepage_url` will not be removed yet as they may help with cleaning of other
features.

```{r remove-duplicate-rows}
# Show the rows for manual inspection.
data %>% filter(name %in% duplicate_name_counts$name)

# Duplicate rows are often indicated by the same home_page
duplicate_homepages <- data %>%
  filter(name %in% duplicate_name_counts$name) %>%
  group_by(homepage_url) %>%
  summarise(n = n()) %>%
  filter(n > 1)

# Show rows that are in the duplicate names and have duplicate homepages. This also brings up rows
# with NA for homepage_url, which are rows that need examining as well.
data %>% filter(homepage_url %in% duplicate_homepages$homepage_url & name %in% duplicate_name_counts$name)

# permalink's of rows that look like bad data from manual inspection.
bad_permalink <- c("/organization/adometry-2", "/organization/ayoxxa-biosystems-2",
"/organization/busportal-2", "/organization/cgtrader-2", "/organization/cue-4", "/organization/gain-fitness",
"/organization/checkpoints", "/organization/lightex-ltd-", "/organization/realync-2",
"/organization/trippeace", "/organization/university-of-rochester-2", "/organization/victiv")
data <- data %>% filter(!(permalink %in% bad_permalink))

# Special case where the permalink is also duplicated, the only way to distinguish between the
# two is based on another field, such as market.
data <- data %>% filter(!(permalink == "/organization/treasure-valley-urology-services" & is.na(market)))

# Check for duplicate permalink values not removed during above process.
data %>% group_by(permalink) %>% summarise(n = n()) %>% filter(n > 1)

# After manually examining the output, use the following to remove the bad row:
data <- data %>% filter(!(permalink == "/organization/prysm" & is.na(market)))
```

### Market and Categories

NA values in `market` will be replaced with the string "None".

@@@ Split by status, then plot histograms of counts of markets in that status. Looking to see if there is a
trend. Counts of how many companies are in that market.

@@@@ Categories, we need to split into components. To use for prediction, need to spread each value of
category becomes a feature. Do a small example of the resulting matrix, but will not actually do it fully as
the size will be huge. Split the categories and print out the number of unique categories, and perhaps the
histogram of the categories. Perhaps a reduced set of categories that occur frequently enough can be used.

```{r removed-unused-features}
#data <- data %>% select(-permalink, -homepage_url)
```

@@@@ Turn status, country, state etc into factors. Dates from strings to datetime.

@@@@ What to use for measuring accuracy/loss function?

## Models

Given the prevalance of companies with the status of opearting is 87%, just guessing the status as operating
would give us this accuracy, but with no ability to predict for the other statuses.

@@@@ section that explains the process and techniques used, including data cleaning, data exploration and visualization, any insights gained, and your modeling approach. At least two different models or algorithms must be used, with at least one being more advanced than simple linear regression for prediction problems.

# Results 

@@@@ A results section that presents the modeling results and discusses the model performance.

# Conclusion

@@@@ A conclusion section that gives a brief summary of the report, its potential impact, its limitations, and future work.


# Bibliography

